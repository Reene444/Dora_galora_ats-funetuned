Traditional LoRA methods, while effective in reducing trainable parameters, have limitations in training efficiency and optimization stability. 
This approach improved training stability while maintaining model performance.

### Usage
` python fine_tune_resume_dora.py`

### Script Overview

The `fine_tune_resume_dora.py` script supports:
- DoRA
- Gradient clipping 
- Float16 precision optimization
- training monitoring and logging


## References

Xutao L. "GaLore+: Boosting Low-Rank Adaptation for LLMs with Cross-Head Projection." Paper under double-blind review. https://www.arxiv.org/pdf/2412.19820




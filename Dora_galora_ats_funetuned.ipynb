{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMbKmqtR/3NrLQWf4lZmetT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Reene444/Dora_galora_ats-funetuned/blob/main/Dora_galora_ats_funetuned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TRAIN_STEPS\"]=\"120\"\n",
        "os.environ[\"MAX_EPOCHS\"]=\"1\"\n",
        "os.environ[\"WARMUP_STEPS\"]=\"8\"\n",
        "os.environ[\"BATCH_SIZE\"]=\"1\"\n",
        "os.environ[\"GRAD_ACCUM_STEPS\"]=\"2\"\n",
        "os.environ[\"DORA_RANK\"]=\"4\"\n",
        "os.environ[\"DORA_ALPHA\"]=\"16\"\n",
        "os.environ[\"DORA_DROPOUT\"]=\"0.05\"\n",
        "os.environ[\"GALORE_ENABLE\"]=\"0\"\n",
        "os.environ[\"SYNTHETIC_SAMPLES_PER_GROUP\"]=\"10\"\n",
        "os.environ[\"STAR_SAMPLES\"]=\"20\"\n",
        "os.environ[\"MAX_INPUT_CHARS\"]=\"6000\"\n",
        "os.environ[\"USE_8BIT\"] = \"1\""
      ],
      "metadata": {
        "id": "yFEsPc5kJzlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZwt0JnD_8wc",
        "outputId": "ca78e4da-685c-45d9-e82d-4304919a71c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upgrading bitsandbytes to latest version...\n",
            "bitsandbytes upgraded successfully\n",
            "Created true DoRAConfig implementation (weight decomposition into magnitude + direction)\n",
            "Detected Google Colab environment, using working directory: /content\n",
            "\n",
            "============================================================\n",
            "Starting DoRA + GaLore Fine-tuning (PyTorch + PEFT)\n",
            "============================================================\n",
            "\n",
            "Working Directory: /content\n",
            "Training Data Directory: /content/torch_data\n",
            "Model Save Directory: /content/fine-tuned-model\n",
            "\n",
            "Created directory: /content/torch_data\n",
            "Created directory: /content/fine-tuned-model\n",
            "\n",
            "Reading resume PDF...\n",
            "Detected Google Colab environment, using working directory: /content\n",
            "Found resume file: /content/content/F_CV_Chen_Linlin_formal_cv_itp.pdf\n",
            "Reading: /content/content/F_CV_Chen_Linlin_formal_cv_itp.pdf\n",
            "   Read page 1 (11619 characters)\n",
            "Resume extraction complete: 11620 characters, 1300 words, 101 lines\n",
            "Generating training data from resume (enhanced/heavy-weight)...\n",
            "Generated samples (including augmentation): 839\n",
            "Generated samples (incl. augmentation/STAR/free QA): 869\n",
            "Creating training file: /content/torch_data/train.jsonl\n",
            "Created 783 training samples\n",
            "Creating training file: /content/torch_data/valid.jsonl\n",
            "Created 86 training samples\n",
            "\n",
            "============================================================\n",
            "Training Data Summary\n",
            "============================================================\n",
            "Training Data Directory: /content/torch_data\n",
            "   ├─ Train file: /content/torch_data/train.jsonl\n",
            "   │  └─ Samples: 783\n",
            "   └─ Validation file: /content/torch_data/valid.jsonl\n",
            "      └─ Samples: 86\n",
            "Model Save Directory: /content/fine-tuned-model\n",
            "Total Training Samples: 783\n",
            "Total Validation Samples: 86\n",
            "============================================================\n",
            "\n",
            "Using device: cuda (dtype=torch.float16)\n",
            "WARNING: bitsandbytes not available or version too old\n",
            "Attempting to upgrade bitsandbytes...\n",
            "bitsandbytes upgraded successfully\n",
            "WARNING: Quantization requested but bitsandbytes unavailable, using float16\n",
            "Loading model with config: {'torch_dtype': torch.float16}\n",
            "Using DoRA configuration\n",
            "Enabled input requires_grad for checkpointing\n",
            "Applying DoRA to 44 LoRA layers...\n",
            "DoRA decomposition applied to 44 layers\n",
            "Applied DoRA weight decomposition (magnitude + direction)\n",
            "Found 44 DoRA magnitude parameters (total: 50688 params)\n",
            "trainable params: 664,576 || all params: 1,100,712,960 || trainable%: 0.0604\n",
            "Enabled gradient checkpointing (memory optimization)\n",
            "GPU memory: 15.83 GB total\n",
            "GPU memory allocated: 4.43 GB\n",
            "GPU memory reserved: 4.61 GB\n",
            "Training plan: epochs=1, steps/epoch≈392, total_steps=392\n",
            "GaLoreAdamW initialized (without rank parameter)\n",
            "\n",
            "Epoch 1/1\n",
            "   step 5/392 - loss: 4.0784\n",
            "   step 10/392 - loss: 3.3493\n",
            "   step 15/392 - loss: 3.6487\n",
            "   step 20/392 - loss: 4.0612\n",
            "   step 25/392 - loss: 6.0965\n",
            "   step 30/392 - loss: 3.8114\n",
            "   step 35/392 - loss: 3.6232\n",
            "   step 40/392 - loss: 4.5710\n",
            "   step 45/392 - loss: 3.7166\n",
            "   step 50/392 - loss: 3.8942\n",
            "   step 55/392 - loss: 5.5668\n",
            "   step 60/392 - loss: 3.5129\n",
            "   step 65/392 - loss: 4.2023\n",
            "   step 70/392 - loss: 4.1045\n",
            "   step 75/392 - loss: 3.5276\n",
            "   step 80/392 - loss: 3.8126\n",
            "   step 85/392 - loss: 3.3558\n",
            "   step 90/392 - loss: 4.6835\n",
            "   step 95/392 - loss: 4.3376\n",
            "   step 100/392 - loss: 3.5840\n",
            "   step 105/392 - loss: 2.7035\n",
            "   step 110/392 - loss: 4.1223\n",
            "   step 115/392 - loss: 4.2112\n",
            "   step 120/392 - loss: 3.9125\n",
            "\n",
            "Training complete! Adapter saved to: /content/fine-tuned-model\n",
            "Final validation loss: nan\n",
            "\n",
            "Fine-tuning successful! You can now use the fine-tuned model\n",
            "Training summary: {\n",
            "  \"train_steps\": 120,\n",
            "  \"validation_loss\": NaN,\n",
            "  \"device\": \"cuda\",\n",
            "  \"dtype\": \"torch.float16\",\n",
            "  \"galore_backend\": \"official\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Fine-tune TinyLlama with DoRA + GaLore for resume-based job applications.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Iterable, List, Optional, Sequence\n",
        "\n",
        "# --- Dependency bootstrap ----------------------------------------------------\n",
        "\n",
        "\n",
        "def _ensure_package(import_name: str, pip_name: Optional[str] = None) -> None:\n",
        "    try:\n",
        "        __import__(import_name)\n",
        "    except ImportError:\n",
        "        target = pip_name or import_name\n",
        "        print(f\"Installing dependency: {target}\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", target])\n",
        "\n",
        "\n",
        "for pkg in [\n",
        "    (\"torch\", \"torch\"),\n",
        "    (\"transformers\", \"transformers>=4.40.0\"),\n",
        "    (\"peft\", \"peft>=0.10.0\"),\n",
        "    (\"accelerate\", \"accelerate>=0.30.0\"),\n",
        "    (\"PyPDF2\", \"pypdf2\"),\n",
        "]:\n",
        "    _ensure_package(*pkg)\n",
        "\n",
        "# Import torch to check CUDA availability\n",
        "import torch\n",
        "\n",
        "# Try to install/upgrade bitsandbytes for quantization (CUDA only, optional)\n",
        "bitsandbytes_available = False\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        import bitsandbytes\n",
        "        # Check if it's a recent version by trying to import the quantizer\n",
        "        try:\n",
        "            from transformers.utils import is_bitsandbytes_available\n",
        "            if is_bitsandbytes_available():\n",
        "                bitsandbytes_available = True\n",
        "                print(\"bitsandbytes is available\")\n",
        "            else:\n",
        "                raise ImportError(\"bitsandbytes version too old\")\n",
        "        except (ImportError, AttributeError):\n",
        "            # Try to upgrade bitsandbytes\n",
        "            print(\"Upgrading bitsandbytes to latest version...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"bitsandbytes\"])\n",
        "            import importlib\n",
        "            importlib.reload(bitsandbytes) if 'bitsandbytes' in sys.modules else None\n",
        "            bitsandbytes_available = True\n",
        "            print(\"bitsandbytes upgraded successfully\")\n",
        "    except ImportError:\n",
        "        try:\n",
        "            print(\"Installing bitsandbytes for quantization support...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"bitsandbytes\"])\n",
        "            import bitsandbytes\n",
        "            bitsandbytes_available = True\n",
        "            print(\"bitsandbytes installed successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Could not install/upgrade bitsandbytes: {e}\")\n",
        "            print(\"   Quantization will be disabled. Model will use more memory.\")\n",
        "            bitsandbytes_available = False\n",
        "else:\n",
        "    print(\"INFO: CUDA not available, skipping bitsandbytes installation (CPU/MPS mode)\")\n",
        "\n",
        "# Try to use official GaLore, fall back to custom implementation if it's not available\n",
        "try:\n",
        "    from galore_torch import GaLoreAdamW  # type: ignore\n",
        "except ImportError:\n",
        "    try:\n",
        "        print(\"Installing GaLore from GitHub...\")\n",
        "        subprocess.check_call(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"git+https://github.com/jiaweizzhao/GaLore.git\"]\n",
        "        )\n",
        "        from galore_torch import GaLoreAdamW  # type: ignore\n",
        "    except Exception as galore_err:\n",
        "        print(f\"WARNING: GaLore official package unavailable: {galore_err}\")\n",
        "        GaLoreAdamW = None  # fall back to custom gradient projection\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    get_cosine_schedule_with_warmup,\n",
        ")\n",
        "from peft import PeftModel, get_peft_model\n",
        "\n",
        "# Try multiple import paths for DoRAConfig (different PEFT versions)\n",
        "# If not available, implement true DoRA (Weight-Decomposed Low-Rank Adaptation)\n",
        "DoRAConfig = None\n",
        "try:\n",
        "    from peft import DoRAConfig\n",
        "    print(\"Using native DoRAConfig from PEFT\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        from peft.tuners.dora import DoRAConfig\n",
        "        print(\"Using DoRAConfig from peft.tuners.dora\")\n",
        "    except ImportError:\n",
        "        try:\n",
        "            from peft.tuners.dora.config import DoRAConfig\n",
        "            print(\"Using DoRAConfig from peft.tuners.dora.config\")\n",
        "        except ImportError:\n",
        "            # Implement true DoRA: Weight decomposition into magnitude and direction\n",
        "            from peft import LoraConfig\n",
        "            from dataclasses import dataclass, field\n",
        "\n",
        "            @dataclass\n",
        "            class DoRAConfig(LoraConfig):\n",
        "                \"\"\"True DoRA (Weight-Decomposed Low-Rank Adaptation) configuration.\n",
        "\n",
        "                DoRA decomposes weights W into magnitude m and direction V:\n",
        "                W = m * V / ||V||_c\n",
        "\n",
        "                Where:\n",
        "                - m: learnable magnitude vector\n",
        "                - V: direction matrix (adapted via LoRA)\n",
        "                - ||V||_c: column-wise L2 norm\n",
        "                \"\"\"\n",
        "                use_dora: bool = field(default=True, metadata={\"help\": \"Enable DoRA weight decomposition\"})\n",
        "\n",
        "            print(\"Created true DoRAConfig implementation (weight decomposition into magnitude + direction)\")\n",
        "\n",
        "# ---------------- Heavy synthetic data controls ----------------\n",
        "SYNTHETIC_SAMPLES_PER_GROUP = int(os.getenv(\"SYNTHETIC_SAMPLES_PER_GROUP\", 120))  # per ATS/type group (raised default)\n",
        "MAX_INPUT_CHARS = int(os.getenv(\"MAX_INPUT_CHARS\", 100000))  # 100K chars to cover full resume (100% coverage)\n",
        "EXHAUSTIVE_MAX_LINES = int(os.getenv(\"EXHAUSTIVE_MAX_LINES\", 10000))  # 10K lines to cover all lines\n",
        "STAR_SAMPLES = int(os.getenv(\"STAR_SAMPLES\", 200))  # number of STAR/free-form samples\n",
        "# 100% coverage flags\n",
        "ENABLE_WORD_LEVEL_COVERAGE = os.getenv(\"WORD_LEVEL_COVERAGE\", \"1\") not in {\"0\", \"false\", \"False\"}\n",
        "ENABLE_SENTENCE_LEVEL_COVERAGE = os.getenv(\"SENTENCE_LEVEL_COVERAGE\", \"1\") not in {\"0\", \"false\", \"False\"}\n",
        "ENABLE_PARAGRAPH_LEVEL_COVERAGE = os.getenv(\"PARAGRAPH_LEVEL_COVERAGE\", \"1\") not in {\"0\", \"false\", \"False\"}\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "def _get_working_directory() -> Path:\n",
        "    \"\"\"Determine the working directory.\n",
        "\n",
        "    If running as a script, use the script's directory.\n",
        "    If in Colab or interactive mode, use the current working directory.\n",
        "    \"\"\"\n",
        "    # Try __file__ first (works when running as a script)\n",
        "    # In Colab notebooks, __file__ doesn't exist, so this will raise NameError\n",
        "    try:\n",
        "        file_path = __file__\n",
        "        return Path(file_path).parent.absolute()\n",
        "    except NameError:\n",
        "        # In Colab or interactive mode, use current directory\n",
        "        cwd = Path(os.getcwd())\n",
        "\n",
        "        # Colab usually uses /content as the working directory\n",
        "        if str(cwd).startswith('/content'):\n",
        "            print(f\"Detected Google Colab environment, using working directory: {cwd}\")\n",
        "            return cwd\n",
        "\n",
        "        # Otherwise use the current directory\n",
        "        print(f\"Using current working directory: {cwd}\")\n",
        "        return cwd\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    model_name: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "    # Use working directory (compatible with local scripts and Google Colab)\n",
        "    _script_dir = _get_working_directory()\n",
        "    adapter_path: str = str(_script_dir / \"fine-tuned-model\")\n",
        "    data_dir: Path = _script_dir / \"torch_data\"\n",
        "    # Reduced defaults for Colab memory constraints\n",
        "    max_seq_length: int = int(os.getenv(\"MAX_SEQ_LENGTH\", 1024))  # Reduced from 2048\n",
        "    per_device_batch_size: int = int(os.getenv(\"BATCH_SIZE\", 1))\n",
        "    grad_accum_steps: int = int(os.getenv(\"GRAD_ACCUM_STEPS\", 8))  # Increased to compensate\n",
        "    learning_rate: float = float(os.getenv(\"LEARNING_RATE\", 5e-6))  # Very conservative default to avoid NaN\n",
        "    weight_decay: float = 0.01\n",
        "    warmup_steps: int = int(os.getenv(\"WARMUP_STEPS\", 50))\n",
        "    target_steps: int = int(os.getenv(\"TRAIN_STEPS\", 100))\n",
        "    max_epochs: int = int(os.getenv(\"MAX_EPOCHS\", 3))\n",
        "    dora_rank: int = int(os.getenv(\"DORA_RANK\", 8))\n",
        "    dora_alpha: int = int(os.getenv(\"DORA_ALPHA\", 64))\n",
        "    dora_dropout: float = float(os.getenv(\"DORA_DROPOUT\", 0.05))\n",
        "    galore_rank_ratio: float = float(os.getenv(\"GALORE_RANK_RATIO\", 0.08))\n",
        "    galore_update_interval: int = int(os.getenv(\"GALORE_UPDATE_INTERVAL\", 1))\n",
        "    galore_project_on: bool = os.getenv(\"GALORE_ENABLE\", \"1\") not in {\"0\", \"false\", \"False\"}\n",
        "    max_grad_norm: float = float(os.getenv(\"MAX_GRAD_NORM\", 0.5))  # Stricter gradient clipping to prevent NaN\n",
        "    use_8bit: bool = os.getenv(\"USE_8BIT\", \"1\") not in {\"0\", \"false\", \"False\"}  # 8-bit quantization\n",
        "    use_4bit: bool = os.getenv(\"USE_4BIT\", \"0\") not in {\"0\", \"false\", \"False\"}  # 4-bit quantization (more aggressive)\n",
        "\n",
        "\n",
        "class ConversationDataset(Dataset):\n",
        "    \"\"\"Dataset turning chat-style messages into supervised fine-tuning tensors.\"\"\"\n",
        "\n",
        "    def __init__(self, samples: Sequence[Dict[str, Any]], tokenizer: AutoTokenizer, max_length: int):\n",
        "        self.samples = samples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        sample = self.samples[idx]\n",
        "        messages = sample.get(\"messages\", [])\n",
        "        if len(messages) < 2:\n",
        "            raise ValueError(\"Sample must contain at least a user and assistant message.\")\n",
        "\n",
        "        # All but final assistant response considered prompt; final message treated as answer.\n",
        "        prompt_messages = messages[:-1]\n",
        "        answer_message = messages[-1]\n",
        "        if answer_message.get(\"role\") != \"assistant\":\n",
        "            # enforce assistant role for target\n",
        "            answer_message = {\"role\": \"assistant\", \"content\": str(answer_message.get(\"content\", \"\"))}\n",
        "\n",
        "        eos_token = self.tokenizer.eos_token or self.tokenizer.pad_token or \"\"\n",
        "\n",
        "        if hasattr(self.tokenizer, \"apply_chat_template\"):\n",
        "            prompt_text = self.tokenizer.apply_chat_template(\n",
        "                prompt_messages, tokenize=False, add_generation_prompt=True\n",
        "            )\n",
        "        else:\n",
        "            prompt_parts = []\n",
        "            for msg in prompt_messages:\n",
        "                role = msg.get(\"role\", \"user\").upper()\n",
        "                prompt_parts.append(f\"{role}: {msg.get('content', '')}\")\n",
        "            prompt_parts.append(\"ASSISTANT:\")\n",
        "            prompt_text = \"\\n\".join(prompt_parts) + \" \"\n",
        "\n",
        "        answer_text = answer_message.get(\"content\", \"\")\n",
        "\n",
        "        prompt_ids = self.tokenizer(\n",
        "            prompt_text,\n",
        "            add_special_tokens=False,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "        )[\"input_ids\"]\n",
        "\n",
        "        answer_suffix = eos_token if eos_token else \"\"\n",
        "        answer_ids = self.tokenizer(\n",
        "            answer_text + answer_suffix,\n",
        "            add_special_tokens=False,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "        )[\"input_ids\"]\n",
        "\n",
        "        input_ids = prompt_ids + answer_ids\n",
        "        input_ids = input_ids[: self.max_length]\n",
        "\n",
        "        # Mask prompt tokens (only supervise assistant response)\n",
        "        labels = [-100] * len(prompt_ids)\n",
        "        labels += answer_ids\n",
        "        labels = labels[: self.max_length]\n",
        "\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_conversations(batch: Sequence[Dict[str, Any]], pad_token_id: int) -> Dict[str, torch.Tensor]:\n",
        "    max_len = max(len(item[\"input_ids\"]) for item in batch)\n",
        "    batch_size = len(batch)\n",
        "\n",
        "    input_ids = torch.full((batch_size, max_len), pad_token_id, dtype=torch.long)\n",
        "    attention_mask = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
        "    labels = torch.full((batch_size, max_len), -100, dtype=torch.long)\n",
        "\n",
        "    for i, item in enumerate(batch):\n",
        "        length = len(item[\"input_ids\"])\n",
        "        input_ids[i, :length] = torch.tensor(item[\"input_ids\"], dtype=torch.long)\n",
        "        attention_mask[i, :length] = 1\n",
        "        labels[i, :length] = torch.tensor(item[\"labels\"], dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def project_gradients_low_rank(\n",
        "    model: nn.Module,\n",
        "    rank_ratio: float = 0.08,\n",
        ") -> None:\n",
        "    \"\"\"Apply GaLore-style low-rank projection to gradients (fallback when GaLoreAdamW unavailable).\"\"\"\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad or param.grad is None:\n",
        "            continue\n",
        "        grad = param.grad.data\n",
        "        if grad.ndim < 2:\n",
        "            # Skip bias / vector params\n",
        "            continue\n",
        "        if grad.is_sparse:\n",
        "            continue\n",
        "\n",
        "        rows, cols = grad.shape[0], grad.reshape(grad.shape[0], -1).shape[1]\n",
        "        min_dim = min(rows, cols)\n",
        "        if min_dim < 2:\n",
        "            continue\n",
        "\n",
        "        rank = max(1, int(min_dim * rank_ratio))\n",
        "        if rank >= min_dim:\n",
        "            continue\n",
        "\n",
        "        grad_matrix = grad.reshape(rows, -1)\n",
        "        try:\n",
        "            u, s, vh = torch.linalg.svd(grad_matrix, full_matrices=False)\n",
        "            s[rank:] = 0\n",
        "            approx = (u[:, :rank] * s[:rank]) @ vh[:rank, :]\n",
        "        except RuntimeError:\n",
        "            q, r = torch.linalg.qr(grad_matrix)\n",
        "            approx = q[:, :rank] @ r[:rank, :]\n",
        "\n",
        "        grad.copy_(approx.reshape_as(grad))\n",
        "\n",
        "def _extract_fields_from_resume(resume_text):\n",
        "    \"\"\"Extract basic fields from resume: email, phone, name, education, experience, skills, etc.\"\"\"\n",
        "    import re\n",
        "    # Find email\n",
        "    email_match = re.search(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", resume_text)\n",
        "    email = email_match.group() if email_match else \"lxc645@alumni.bham.ac.uk\"\n",
        "    # Find phone number\n",
        "    phone_match = re.search(r\"(\\+?[\\d\\s\\-\\(\\)]{10,})\", resume_text)\n",
        "    phone = phone_match.group().strip() if phone_match else \"+447827396618\"\n",
        "    # Name is usually the first non-empty line\n",
        "    first_line = next((ln.strip() for ln in resume_text.split(\"\\n\") if ln.strip()), \"Linlin Chen\")\n",
        "    name = first_line\n",
        "    # Split into first/last name\n",
        "    parts = name.split()\n",
        "    first_name = parts[0] if parts else \"Linlin\"\n",
        "    last_name = parts[-1] if len(parts) > 1 else \"Chen\"\n",
        "    # Try to guess education level (not very accurate, but good enough)\n",
        "    edu = \"Master's degree\" if re.search(r\"Master|MSc|MS|MA\", resume_text, re.I) else (\"Bachelor's degree\" if re.search(r\"Bachelor|BSc|BA\", resume_text, re.I) else \"\")\n",
        "    # Try to find years of experience\n",
        "    years = 2\n",
        "    m_years = re.findall(r\"(\\d+)\\+?\\s*(years|yrs)\", resume_text, re.I)\n",
        "    if m_years:\n",
        "        try:\n",
        "            years = min(10, max(1, int(m_years[0][0])))\n",
        "        except:\n",
        "            pass\n",
        "    # Look for common skills and languages\n",
        "    skills = []\n",
        "    for kw in [\"Python\", \"AI/ML\", \"Machine Learning\", \"Deep Learning\", \"NLP\", \"LLM\", \"Full Stack\", \"Selenium\", \"OpenAI\", \"Ollama\"]:\n",
        "        if re.search(rf\"\\b{re.escape(kw)}\\b\", resume_text, re.I):\n",
        "            skills.append(kw)\n",
        "    if not skills:\n",
        "        skills = [\"Python\", \"AI/ML\", \"Full Stack Development\"]\n",
        "    languages = []\n",
        "    for lang in [\"English\", \"Chinese\", \"Mandarin\", \"Cantonese\"]:\n",
        "        if re.search(rf\"\\b{re.escape(lang)}\\b\", resume_text, re.I):\n",
        "            languages.append(lang)\n",
        "    if not languages:\n",
        "        languages = [\"English\", \"Chinese\"]\n",
        "    summary = resume_text[:600]\n",
        "    return {\n",
        "        \"name\": name,\n",
        "        \"first_name\": first_name,\n",
        "        \"last_name\": last_name,\n",
        "        \"email\": email,\n",
        "        \"phone\": phone,\n",
        "        \"education\": edu,\n",
        "        \"experience_years\": years,\n",
        "        \"skills\": skills,\n",
        "        \"languages\": languages,\n",
        "        \"summary\": summary,\n",
        "    }\n",
        "\n",
        "def _generate_exhaustive_line_samples(resume_text: str) -> list[dict]:\n",
        "    \"\"\"Synthesize per-line/word/sentence/paragraph coverage samples for 100% resume coverage.\"\"\"\n",
        "    import re\n",
        "    lines = [ln.strip() for ln in resume_text.split('\\n') if ln.strip()]\n",
        "    # Use ALL lines (no truncation for 100% coverage)\n",
        "    lines = lines[:min(len(lines), EXHAUSTIVE_MAX_LINES)]\n",
        "\n",
        "    def msg(user, assistant):\n",
        "        return {\"messages\": [{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}]}\n",
        "\n",
        "    samples: list[dict] = []\n",
        "\n",
        "    # 1. Verbatim line reproduction (100% line coverage)\n",
        "    for ln in lines:\n",
        "        samples.append(msg(\"Return this resume line verbatim (output only the original line):\", ln))\n",
        "\n",
        "    # 2. Key-value parsing for structured lines\n",
        "    kv_pattern = re.compile(r\"^([A-Za-z \\-_/]+):\\s*(.+)$\")\n",
        "    for ln in lines:\n",
        "        m = kv_pattern.match(ln)\n",
        "        if m:\n",
        "            key = m.group(1).strip()\n",
        "            val = m.group(2).strip()\n",
        "            samples.append(msg(\n",
        "                f\"Parse this line into a JSON key-value (output JSON only): {ln}\",\n",
        "                json.dumps({key: val}, ensure_ascii=False)\n",
        "            ))\n",
        "\n",
        "    # 3. Word-level coverage (if enabled)\n",
        "    if ENABLE_WORD_LEVEL_COVERAGE:\n",
        "        words = re.findall(r'\\b\\w+\\b', resume_text)\n",
        "        # Sample every Nth word to avoid too many samples, but ensure coverage\n",
        "        word_sample_rate = max(1, len(words) // 500)  # ~500 word samples max\n",
        "        for i in range(0, len(words), word_sample_rate):\n",
        "            word = words[i]\n",
        "            if len(word) > 2:  # Skip very short words\n",
        "                word_lower = word.lower()\n",
        "                resume_lower = resume_text.lower()\n",
        "                word_pos = resume_lower.find(word_lower)\n",
        "                if word_pos >= 0:\n",
        "                    context_start = max(0, word_pos - 50)\n",
        "                    context_end = min(len(resume_text), word_pos + len(word) + 50)\n",
        "                    context = resume_text[context_start:context_end]\n",
        "                    samples.append(msg(\n",
        "                        f\"Does my resume contain the word '{word}'? If yes, provide context (one sentence).\",\n",
        "                        f\"Yes, '{word}' appears in my resume. Context: {context}\"\n",
        "                    ))\n",
        "\n",
        "    # 4. Sentence-level coverage (if enabled)\n",
        "    if ENABLE_SENTENCE_LEVEL_COVERAGE:\n",
        "        sentences = re.split(r'[.!?]+', resume_text)\n",
        "        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
        "        for sent in sentences[:min(500, len(sentences))]:  # Limit to 500 sentences\n",
        "            if len(sent) > 10:\n",
        "                samples.append(msg(\n",
        "                    \"Return this sentence from my resume verbatim:\",\n",
        "                    sent\n",
        "                ))\n",
        "\n",
        "    # 5. Paragraph-level coverage (if enabled)\n",
        "    if ENABLE_PARAGRAPH_LEVEL_COVERAGE:\n",
        "        paragraphs = [p.strip() for p in resume_text.split('\\n\\n') if len(p.strip()) > 20]\n",
        "        for para in paragraphs[:min(200, len(paragraphs))]:  # Limit to 200 paragraphs\n",
        "            samples.append(msg(\n",
        "                \"Return this paragraph from my resume verbatim:\",\n",
        "                para\n",
        "            ))\n",
        "            # Also extract structured info from paragraphs\n",
        "            samples.append(msg(\n",
        "                f\"Extract all key information from this resume paragraph as JSON: {para[:200]}...\",\n",
        "                json.dumps({\"paragraph_content\": para[:500]}, ensure_ascii=False)\n",
        "            ))\n",
        "\n",
        "    return samples\n",
        "\n",
        "def generate_training_data(resume_text):\n",
        "    \"\"\"Generate training samples from resume. Creates lots of examples for form filling and ATS scenarios.\"\"\"\n",
        "    print(\"Generating training data from resume (enhanced/heavy-weight)...\")\n",
        "    fields = _extract_fields_from_resume(resume_text)\n",
        "\n",
        "    def msg(user, assistant):\n",
        "        return {\"messages\": [{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}]}\n",
        "\n",
        "    samples: list[dict] = []\n",
        "\n",
        "    # Cover every line of the resume\n",
        "    samples.extend(_generate_exhaustive_line_samples(resume_text))\n",
        "\n",
        "    # Extract all the basic fields and create Q&A pairs\n",
        "    base_kv = {\n",
        "        \"firstName\": fields[\"first_name\"],\n",
        "        \"lastName\": fields[\"last_name\"],\n",
        "        \"email\": fields[\"email\"],\n",
        "        \"phone\": fields[\"phone\"],\n",
        "        \"education\": fields[\"education\"] or \"Master's degree\",\n",
        "        \"experience_years\": fields[\"experience_years\"],\n",
        "        \"skills\": fields[\"skills\"],\n",
        "        \"languages\": fields[\"languages\"],\n",
        "        \"summary\": fields[\"summary\"][:MAX_INPUT_CHARS],\n",
        "        \"linkedin\": \"linkedin.com/in/lchen198\",\n",
        "        \"github\": \"github.com/Reene444\",\n",
        "        \"website\": \"https://reene4444.com/\",\n",
        "    }\n",
        "\n",
        "    import json as _json\n",
        "\n",
        "    # Create simple Q&A for each field, plus JSON mapping\n",
        "    field_aliases = {\n",
        "        \"first name\": base_kv[\"firstName\"],\n",
        "        \"given_name\": base_kv[\"firstName\"],\n",
        "        \"family name\": base_kv[\"lastName\"],\n",
        "        \"last name\": base_kv[\"lastName\"],\n",
        "        \"email\": base_kv[\"email\"],\n",
        "        \"e_mail\": base_kv[\"email\"],\n",
        "        \"phone\": base_kv[\"phone\"],\n",
        "        \"mobile\": base_kv[\"phone\"],\n",
        "        \"linkedin\": base_kv[\"linkedin\"],\n",
        "        \"github\": base_kv[\"github\"],\n",
        "        \"website\": base_kv[\"website\"],\n",
        "    }\n",
        "\n",
        "    for alias, val in field_aliases.items():\n",
        "        samples.append(msg(f\"Form field to fill: {alias}?\\nOutput answer only.\", str(val)))\n",
        "        samples.append(msg(f\"Analyze HTML input: <input name='{alias}'>. What value should be filled?\\nOutput answer only.\", str(val)))\n",
        "\n",
        "    samples.append(msg(\n",
        "        \"Extract from resume and return JSON (output JSON only): {firstName,lastName,email,phone,education,experience_years,skills,languages,summary}\",\n",
        "        _json.dumps({k: base_kv[k] for k in [\"firstName\",\"lastName\",\"email\",\"phone\",\"education\",\"experience_years\",\"skills\",\"languages\",\"summary\"]}, ensure_ascii=False)\n",
        "    ))\n",
        "\n",
        "    # Create samples for different ATS platforms (Greenhouse, Workday, Lever, etc.)\n",
        "    ats_types = [\"greenhouse\", \"workday\", \"lever\", \"generic_form\"]\n",
        "\n",
        "    action_plan_template = {\n",
        "        \"greenhouse\": [\n",
        "            \"Detect site type = greenhouse\",\n",
        "            \"Locate external apply link (avoid 'Easy Apply')\",\n",
        "            \"Click external apply button (opens new tab)\",\n",
        "            \"Switch to new tab\",\n",
        "            \"Wait for form to load\",\n",
        "            \"Fill fields using resume mapping\",\n",
        "            \"Upload resume if required\",\n",
        "            \"Submit or continue to next step\",\n",
        "        ],\n",
        "        \"workday\": [\n",
        "            \"Detect site type = workday\",\n",
        "            \"Click 'Apply' or 'Apply Now'\",\n",
        "            \"Handle authentication modal if present (skip if not)\",\n",
        "            \"Navigate to personal info section\",\n",
        "            \"Fill fields using resume mapping\",\n",
        "            \"Upload resume\",\n",
        "            \"Review and submit\",\n",
        "        ],\n",
        "        \"lever\": [\n",
        "            \"Detect site type = lever\",\n",
        "            \"Click 'Apply for this job'\",\n",
        "            \"Wait for form iframe or modal\",\n",
        "            \"Fill fields using resume mapping\",\n",
        "            \"Upload resume\",\n",
        "            \"Submit\",\n",
        "        ],\n",
        "        \"generic_form\": [\n",
        "            \"Detect site type = generic_form\",\n",
        "            \"Find any button containing 'Apply' (exclude 'Easy Apply')\",\n",
        "            \"Click and wait for form\",\n",
        "            \"Fill fields using resume mapping\",\n",
        "            \"Submit\",\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    selector_map = {\n",
        "        \"first_name\": [\"input[name='first_name']\", \"input#first_name\", \"input[placeholder*='first']\"],\n",
        "        \"last_name\": [\"input[name='last_name']\", \"input#last_name\", \"input[placeholder*='last']\"],\n",
        "        \"email\": [\"input[type='email']\", \"input[name='email']\", \"input[placeholder*='email']\"],\n",
        "        \"phone\": [\"input[type='tel']\", \"input[name='phone']\", \"input[placeholder*='phone']\"],\n",
        "        \"resume\": [\"input[type='file'][name*='resume']\", \"input[name='file']\"],\n",
        "    }\n",
        "\n",
        "    def build_action_json(ats):\n",
        "        return {\n",
        "            \"page_type\": ats,\n",
        "            \"form_fields\": [\n",
        "                {\"field_name\": \"first_name\", \"field_type\": \"text\", \"selector\": selector_map[\"first_name\"][0], \"required\": True, \"suggested_value\": base_kv[\"firstName\"]},\n",
        "                {\"field_name\": \"last_name\", \"field_type\": \"text\", \"selector\": selector_map[\"last_name\"][0], \"required\": True, \"suggested_value\": base_kv[\"lastName\"]},\n",
        "                {\"field_name\": \"email\", \"field_type\": \"email\", \"selector\": selector_map[\"email\"][0], \"required\": True, \"suggested_value\": base_kv[\"email\"]},\n",
        "                {\"field_name\": \"phone\", \"field_type\": \"phone\", \"selector\": selector_map[\"phone\"][0], \"required\": True, \"suggested_value\": base_kv[\"phone\"]},\n",
        "                {\"field_name\": \"resume\", \"field_type\": \"file\", \"selector\": selector_map[\"resume\"][0], \"required\": False, \"suggested_value\": \"<path-to-resume>\"},\n",
        "            ],\n",
        "            \"submit_button\": {\"selector\": \"button[type='submit']\", \"text\": \"Submit\"},\n",
        "            \"action_plan\": action_plan_template[ats],\n",
        "            \"confidence\": 0.9\n",
        "        }\n",
        "\n",
        "    page_stub = (\n",
        "        \"Analyze the job application page (HTML omitted). Using my resume context, output JSON with:\\n\"\n",
        "        \"- page_type (greenhouse|workday|lever|generic_form)\\n\"\n",
        "        \"- form_fields: [{field_name, field_type, selector, required, suggested_value}]\\n\"\n",
        "        \"- submit_button: {selector, text}\\n\"\n",
        "        \"- action_plan: [step1, step2, ...]\\n\"\n",
        "        \"Output JSON only, no extra text.\"\n",
        "    )\n",
        "\n",
        "    for ats in ats_types:\n",
        "        samples.append(msg(page_stub, _json.dumps(build_action_json(ats), ensure_ascii=False)))\n",
        "        for i in range(SYNTHETIC_SAMPLES_PER_GROUP):\n",
        "            btn_text = [\"Apply\", \"Submit\", \"Send\", \"Continue\"][i % 4]\n",
        "            var = build_action_json(ats)\n",
        "            var[\"submit_button\"][\"text\"] = btn_text\n",
        "            if i % 3 == 0:\n",
        "                var[\"form_fields\"].insert(0, var[\"form_fields\"].pop())\n",
        "            if i % 5 == 0:\n",
        "                var[\"form_fields\"].append({\n",
        "                    \"field_name\": \"linkedin\", \"field_type\": \"text\", \"selector\": \"input[name='linkedin']\",\n",
        "                    \"required\": False, \"suggested_value\": base_kv[\"linkedin\"]\n",
        "                })\n",
        "            samples.append(msg(page_stub, _json.dumps(var, ensure_ascii=False)))\n",
        "\n",
        "    robustness_user = (\n",
        "        \"If the page has both 'Easy Apply' and an external 'Apply' button, which should be used? Output 'external' or 'easy' only.\"\n",
        "    )\n",
        "    samples.append(msg(robustness_user, \"external\"))\n",
        "\n",
        "    need_click_more_user = (\n",
        "        \"Form is not visible initially. Which element should be clicked to reveal it? Output one CSS selector only.\"\n",
        "    )\n",
        "    samples.append(msg(need_click_more_user, \"button:contains('Apply')\"))\n",
        "\n",
        "    full_form_user = (\n",
        "        \"Output JSON values for autofill (output JSON only): {first_name,last_name,email,phone,linkedin,github,website,education,experience_years,skills}\"\n",
        "    )\n",
        "    full_form_out = _json.dumps({\n",
        "        \"first_name\": base_kv[\"firstName\"],\n",
        "        \"last_name\": base_kv[\"lastName\"],\n",
        "        \"email\": base_kv[\"email\"],\n",
        "        \"phone\": base_kv[\"phone\"],\n",
        "        \"linkedin\": base_kv[\"linkedin\"],\n",
        "        \"github\": base_kv[\"github\"],\n",
        "        \"website\": base_kv[\"website\"],\n",
        "        \"education\": base_kv[\"education\"],\n",
        "        \"experience_years\": base_kv[\"experience_years\"],\n",
        "        \"skills\": base_kv[\"skills\"],\n",
        "    }, ensure_ascii=False)\n",
        "    samples.append(msg(full_form_user, full_form_out))\n",
        "\n",
        "    print(f\"Generated samples (including augmentation): {len(samples)}\")\n",
        "    # 5) Experience synthesis / Project details / STAR narratives & free-form QA coverage\n",
        "    # Use resume summary/full text slices to generate STAR, project details, strengths/weaknesses, quantified outcomes, timeline, responsibilities, etc.\n",
        "    def msg(user, assistant):\n",
        "        return {\"messages\": [{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}]}\n",
        "\n",
        "    star_prompts = [\n",
        "        \"Summarize a representative project using the STAR method (Situation, Task, Action, Result). Output a single paragraph.\",\n",
        "        \"List 3-5 bullet points for my core experience. Start each with a verb and quantify results where possible.\",\n",
        "        \"Summarize my tech stack and proficiency (concise bullets).\",\n",
        "        \"Extract transferable skills from my experience (e.g., Communication, Leadership, Ownership, Problem Solving).\",\n",
        "        \"Output my project list as JSON array: name, objective, my role, key tech, outcome.\",\n",
        "        \"Provide a 30-second self-introduction focusing on AI/Agents/form automation.\",\n",
        "        \"In first person: What was your most challenging project? What did you do and what was the result?\",\n",
        "        \"Summarize a 3-year timeline (year-company/project-responsibilities-outcomes) as a JSON array.\",\n",
        "        \"Generate standard ATS answers as JSON (visa, availability, remote, salary expectations).\",\n",
        "        \"List 3 role-aligned achievements with measurable metrics (KPI/speed/accuracy).\",\n",
        "    ]\n",
        "\n",
        "    # Generate STAR method samples using the full resume text (no truncation)\n",
        "    knowledge_blob = resume_text[:MAX_INPUT_CHARS]  # MAX_INPUT_CHARS is 100K, so this covers the whole resume\n",
        "    for i in range(min(STAR_SAMPLES, len(star_prompts))):\n",
        "        prompt = star_prompts[i % len(star_prompts)] + \"\\n\\nFull resume content:\\n\" + knowledge_blob\n",
        "        # Use the full resume as the answer - don't truncate\n",
        "        synthesized_answer = knowledge_blob\n",
        "        samples.append(msg(prompt, synthesized_answer))\n",
        "\n",
        "    # Add free-form Q&A templates so the model can answer random questions based on the resume\n",
        "    free_qa_templates = [\n",
        "        \"Answer based on my resume only: {q} (no external knowledge).\",\n",
        "        \"Using my experience only, answer concisely: {q} (say 'unknown' if not present).\",\n",
        "        \"Answer using resume highlights only: {q} (do not fabricate).\",\n",
        "    ]\n",
        "    generic_questions = [\n",
        "        \"You recently did an automation/Agent-related work?\",\n",
        "        \"What are your top three skills?\",\n",
        "        \"Describe a case where you improved efficiency (include metrics).\",\n",
        "        \"Which languages and frameworks are you familiar with?\",\n",
        "        \"Your education background and graduation time?\",\n",
        "        \"If you need to fill first_name/last_name/email/phone, what are these values?\",\n",
        "        \"What are your common responsibilities?\",\n",
        "        \"List your most proud achievements and their impact.\",\n",
        "        \"Do you have any open-source/portfolio links?\",\n",
        "        \"Your visa and available time for employment?\",\n",
        "    ]\n",
        "    for i in range(STAR_SAMPLES):\n",
        "        q = generic_questions[i % len(generic_questions)]\n",
        "        tpl = free_qa_templates[i % len(free_qa_templates)]\n",
        "        prompt = tpl.format(q=q) + \"\\n\\nFull resume content:\\n\" + knowledge_blob\n",
        "        # Use the full resume as answer - don't truncate anything\n",
        "        answer = knowledge_blob\n",
        "        samples.append(msg(prompt, answer))\n",
        "\n",
        "    print(f\"Generated samples (incl. augmentation/STAR/free QA): {len(samples)}\")\n",
        "    return samples\n",
        "\n",
        "def create_training_jsonl(data, output_file=\"training_data.jsonl\"):\n",
        "    \"\"\"Create JSONL file for training\"\"\"\n",
        "    print(f\"Creating training file: {output_file}\")\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        for item in data:\n",
        "            line = json.dumps(item, ensure_ascii=False)\n",
        "            f.write(line + '\\n')\n",
        "\n",
        "    print(f\"Created {len(data)} training samples\")\n",
        "    return output_file\n",
        "\n",
        "def _resolve_device() -> tuple[torch.device, str]:\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\"), \"mps\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\"), \"cuda\"\n",
        "    return torch.device(\"cpu\"), \"cpu\"\n",
        "\n",
        "\n",
        "def _apply_dora_decomposition(model: PeftModel, device: torch.device) -> None:\n",
        "    \"\"\"Apply true DoRA (Weight-Decomposed Low-Rank Adaptation) to LoRA layers.\n",
        "\n",
        "    DoRA decomposes weights W into magnitude m and direction V:\n",
        "    W = m * V / ||V||_c\n",
        "\n",
        "    This function:\n",
        "    1. Finds all LoRA layers in the model\n",
        "    2. Adds magnitude parameters for each LoRA layer\n",
        "    3. Wraps the forward method to apply DoRA decomposition\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from peft.tuners.lora import Linear, LoraLayer\n",
        "    except ImportError:\n",
        "        try:\n",
        "            from peft.tuners.lora.layer import Linear, LoraLayer\n",
        "        except ImportError:\n",
        "            print(\"WARNING: Cannot import LoRA layers, skipping DoRA decomposition\")\n",
        "            return\n",
        "\n",
        "    dora_layers = []\n",
        "\n",
        "    def _find_lora_layers(module, prefix=\"\"):\n",
        "        \"\"\"Recursively find all LoRA layers.\"\"\"\n",
        "        for name, child in module.named_children():\n",
        "            full_name = f\"{prefix}.{name}\" if prefix else name\n",
        "            if isinstance(child, (Linear, LoraLayer)):\n",
        "                dora_layers.append((full_name, child))\n",
        "            else:\n",
        "                _find_lora_layers(child, full_name)\n",
        "\n",
        "    _find_lora_layers(model)\n",
        "\n",
        "    if not dora_layers:\n",
        "        print(\"WARNING: No LoRA layers found, skipping DoRA decomposition\")\n",
        "        return\n",
        "\n",
        "    print(f\"Applying DoRA to {len(dora_layers)} LoRA layers...\")\n",
        "\n",
        "    for layer_name, lora_layer in dora_layers:\n",
        "        # Get the base weight shape to determine output features\n",
        "        try:\n",
        "            if hasattr(lora_layer, 'base_layer'):\n",
        "                base_weight = lora_layer.base_layer.weight\n",
        "            elif hasattr(lora_layer, 'weight'):\n",
        "                base_weight = lora_layer.weight\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            # Create magnitude parameter (one per output dimension)\n",
        "            # For linear layers: weight shape is (out_features, in_features)\n",
        "            out_features = base_weight.shape[0]\n",
        "            # Initialize magnitude to a very small value (0.01) to avoid numerical instability\n",
        "            # Starting from 1.0 or even 0.1 can cause immediate NaN when combined with LoRA updates\n",
        "            # Using 0.01 allows very gradual learning of magnitude scaling\n",
        "            # Alternative: initialize based on LoRA alpha scaling\n",
        "            magnitude_init_value = 0.01  # Very small initial value\n",
        "            magnitude_init = torch.full(\n",
        "                (out_features,),\n",
        "                magnitude_init_value,\n",
        "                device=device,\n",
        "                dtype=base_weight.dtype\n",
        "            )\n",
        "            magnitude = nn.Parameter(magnitude_init, requires_grad=True)\n",
        "\n",
        "            # Store magnitude in the layer\n",
        "            lora_layer.register_parameter('dora_magnitude', magnitude)\n",
        "\n",
        "            # Wrap the forward method to apply DoRA\n",
        "            original_forward = lora_layer.forward\n",
        "\n",
        "            def make_dora_forward(orig_fwd, mag_param, layer_nm):\n",
        "                def dora_forward(*args, **kwargs):\n",
        "                    # Call original forward to get LoRA output\n",
        "                    output = orig_fwd(*args, **kwargs)\n",
        "\n",
        "                    # Apply DoRA: scale by magnitude on the output feature dimension\n",
        "                    # Use torch operations that preserve gradients\n",
        "                    if isinstance(output, torch.Tensor) and output.dim() >= 2:\n",
        "                        # Get the last dimension (feature dimension)\n",
        "                        feature_dim = output.shape[-1]\n",
        "                        mag_size = mag_param.shape[0]\n",
        "\n",
        "                        # Only apply if dimensions match\n",
        "                        if feature_dim == mag_size:\n",
        "                            # Clamp magnitude to prevent extreme values that cause NaN\n",
        "                            # This adds numerical stability\n",
        "                            mag_clamped = torch.clamp(mag_param, min=1e-6, max=10.0)\n",
        "\n",
        "                            # Reshape magnitude to broadcast correctly\n",
        "                            # Use view() which preserves gradients\n",
        "                            view_shape = [1] * (output.dim() - 1) + [mag_size]\n",
        "                            mag_view = mag_clamped.view(*view_shape)\n",
        "\n",
        "                            # Check for NaN/Inf in output before applying DoRA\n",
        "                            if torch.any(torch.isnan(output)) or torch.any(torch.isinf(output)):\n",
        "                                # If output already has NaN, don't apply DoRA scaling\n",
        "                                return output\n",
        "\n",
        "                            # Use mul() to ensure gradient flow\n",
        "                            output_scaled = torch.mul(output, mag_view)\n",
        "\n",
        "                            # Check for NaN/Inf after scaling\n",
        "                            if torch.any(torch.isnan(output_scaled)) or torch.any(torch.isinf(output_scaled)):\n",
        "                                # If scaling caused NaN, return original output\n",
        "                                return output\n",
        "\n",
        "                            return output_scaled\n",
        "                        # Silently skip if dimensions don't match (may happen with reshaped outputs)\n",
        "\n",
        "                    return output\n",
        "                return dora_forward\n",
        "\n",
        "            lora_layer.forward = make_dora_forward(original_forward, magnitude, layer_name)\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Skipping DoRA for layer {layer_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"DoRA decomposition applied to {len(dora_layers)} layers\")\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    model: PeftModel,\n",
        "    dataloader: Optional[DataLoader],\n",
        "    device: torch.device,\n",
        "    device_type: str,\n",
        "    autocast_dtype: torch.dtype,\n",
        ") -> Optional[float]:\n",
        "    if dataloader is None or len(dataloader) == 0:\n",
        "        return None\n",
        "\n",
        "    model.eval()\n",
        "    losses: List[float] = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            with torch.autocast(device_type=device_type, dtype=autocast_dtype, enabled=device_type != \"cpu\"):\n",
        "                outputs = model(**batch)\n",
        "                loss = outputs.loss\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    model.train()\n",
        "    return float(sum(losses) / max(len(losses), 1))\n",
        "\n",
        "\n",
        "def train_with_dora_galore(\n",
        "    train_samples: Sequence[Dict[str, Any]],\n",
        "    valid_samples: Sequence[Dict[str, Any]],\n",
        "    cfg: TrainingConfig,\n",
        ") -> Dict[str, Any]:\n",
        "    device, device_type = _resolve_device()\n",
        "    autocast_dtype = torch.float16 if device_type in {\"cuda\", \"mps\"} else torch.float32\n",
        "\n",
        "    print(f\"Using device: {device} (dtype={autocast_dtype})\")\n",
        "\n",
        "    # Clear CUDA cache before loading model\n",
        "    if device_type == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "        import gc\n",
        "        gc.collect()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token or tokenizer.unk_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    # Prepare quantization config if needed\n",
        "    quantization_config = None\n",
        "    load_kwargs = {}\n",
        "\n",
        "    if device_type == \"cuda\":\n",
        "        # Check if bitsandbytes is available before trying to use quantization\n",
        "        try:\n",
        "            from transformers.utils import is_bitsandbytes_available\n",
        "            bnb_available = is_bitsandbytes_available()\n",
        "        except (ImportError, AttributeError):\n",
        "            # Fallback: try to import bitsandbytes directly\n",
        "            try:\n",
        "                import bitsandbytes\n",
        "                bnb_available = True\n",
        "            except ImportError:\n",
        "                bnb_available = False\n",
        "\n",
        "        if not bnb_available and (cfg.use_4bit or cfg.use_8bit):\n",
        "            print(\"WARNING: bitsandbytes not available or version too old\")\n",
        "            print(\"Attempting to upgrade bitsandbytes...\")\n",
        "            try:\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"bitsandbytes\"])\n",
        "                # Reload to check again\n",
        "                try:\n",
        "                    from transformers.utils import is_bitsandbytes_available\n",
        "                    bnb_available = is_bitsandbytes_available()\n",
        "                except:\n",
        "                    import bitsandbytes\n",
        "                    bnb_available = True\n",
        "                print(\"bitsandbytes upgraded successfully\")\n",
        "            except Exception as upgrade_err:\n",
        "                print(f\"ERROR: Failed to upgrade bitsandbytes: {upgrade_err}\")\n",
        "                print(\"   Falling back to float16 (no quantization)\")\n",
        "                cfg.use_4bit = False\n",
        "                cfg.use_8bit = False\n",
        "                bnb_available = False\n",
        "\n",
        "        if bnb_available:\n",
        "            if cfg.use_4bit:\n",
        "                try:\n",
        "                    from transformers import BitsAndBytesConfig\n",
        "                    quantization_config = BitsAndBytesConfig(\n",
        "                        load_in_4bit=True,\n",
        "                        bnb_4bit_compute_dtype=torch.float16,\n",
        "                        bnb_4bit_use_double_quant=True,\n",
        "                        bnb_4bit_quant_type=\"nf4\"\n",
        "                    )\n",
        "                    load_kwargs[\"quantization_config\"] = quantization_config\n",
        "                    load_kwargs[\"device_map\"] = \"auto\"\n",
        "                    print(\"Using 4-bit quantization (most memory efficient)\")\n",
        "                except Exception as e:\n",
        "                    print(f\"WARNING: 4-bit quantization failed: {e}, falling back to 8-bit\")\n",
        "                    cfg.use_4bit = False\n",
        "                    cfg.use_8bit = True\n",
        "            elif cfg.use_8bit:\n",
        "                try:\n",
        "                    from transformers import BitsAndBytesConfig\n",
        "                    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "                    load_kwargs[\"quantization_config\"] = quantization_config\n",
        "                    load_kwargs[\"device_map\"] = \"auto\"\n",
        "                    print(\"Using 8-bit quantization (memory efficient)\")\n",
        "                except Exception as e:\n",
        "                    print(f\"WARNING: 8-bit quantization failed: {e}, using float16\")\n",
        "                    cfg.use_8bit = False\n",
        "        else:\n",
        "            if cfg.use_4bit or cfg.use_8bit:\n",
        "                print(\"WARNING: Quantization requested but bitsandbytes unavailable, using float16\")\n",
        "                cfg.use_4bit = False\n",
        "                cfg.use_8bit = False\n",
        "\n",
        "    model_dtype = torch.float16 if device_type in {\"cuda\", \"mps\"} and not cfg.use_8bit and not cfg.use_4bit else torch.float32\n",
        "    if quantization_config is None:\n",
        "        load_kwargs[\"torch_dtype\"] = model_dtype\n",
        "\n",
        "    print(f\"Loading model with config: {load_kwargs}\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        cfg.model_name,\n",
        "        **load_kwargs\n",
        "    )\n",
        "\n",
        "    # Only resize if not using quantization (quantized models handle this differently)\n",
        "    if quantization_config is None:\n",
        "        base_model.resize_token_embeddings(len(tokenizer))\n",
        "    else:\n",
        "        # For quantized models, we need to ensure token embeddings are resized\n",
        "        # but this might be handled automatically\n",
        "        try:\n",
        "            base_model.resize_token_embeddings(len(tokenizer))\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Could not resize token embeddings (may be handled automatically): {e}\")\n",
        "\n",
        "    # Always use DoRA configuration\n",
        "    adapter_config = DoRAConfig(\n",
        "        r=cfg.dora_rank,\n",
        "        lora_alpha=cfg.dora_alpha,\n",
        "        lora_dropout=cfg.dora_dropout,\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    print(\"Using DoRA configuration\")\n",
        "\n",
        "    model: PeftModel = get_peft_model(base_model, adapter_config)\n",
        "\n",
        "    # Ensure inputs require grad when using gradient checkpointing (fixes no-grad backward)\n",
        "    if hasattr(model, \"enable_input_require_grads\"):\n",
        "        try:\n",
        "            model.enable_input_require_grads()\n",
        "            print(\"Enabled input requires_grad for checkpointing\")\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Could not enable input requires_grad: {e}\")\n",
        "\n",
        "    # Apply true DoRA: Add magnitude parameters and modify forward logic\n",
        "    if not hasattr(adapter_config, 'use_dora') or adapter_config.use_dora:\n",
        "        _apply_dora_decomposition(model, device)\n",
        "        print(\"Applied DoRA weight decomposition (magnitude + direction)\")\n",
        "\n",
        "        # Verify DoRA magnitude parameters are trainable\n",
        "        dora_params = [p for name, p in model.named_parameters() if 'dora_magnitude' in name and p.requires_grad]\n",
        "        if dora_params:\n",
        "            print(f\"Found {len(dora_params)} DoRA magnitude parameters (total: {sum(p.numel() for p in dora_params)} params)\")\n",
        "\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # Enable gradient checkpointing to save memory (if available)\n",
        "    # This trades compute for memory, which is crucial for Colab\n",
        "    # Note: Gradient checkpointing can sometimes cause numerical instability with custom DoRA\n",
        "    # If NaN persists, try disabling it by setting DISABLE_GRAD_CHECKPOINT=1\n",
        "    disable_checkpoint = os.getenv(\"DISABLE_GRAD_CHECKPOINT\", \"0\") not in {\"0\", \"false\", \"False\"}\n",
        "    if not disable_checkpoint and hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "        try:\n",
        "            model.gradient_checkpointing_enable()\n",
        "            print(\"Enabled gradient checkpointing (memory optimization)\")\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Could not enable gradient checkpointing: {e}\")\n",
        "    else:\n",
        "        if hasattr(model, \"gradient_checkpointing_disable\"):\n",
        "            model.gradient_checkpointing_disable()\n",
        "            if disable_checkpoint:\n",
        "                print(\"INFO: Gradient checkpointing disabled (may help with NaN stability)\")\n",
        "\n",
        "    # Only move to device if not using device_map=\"auto\" (quantization handles this)\n",
        "    if \"device_map\" not in load_kwargs or load_kwargs.get(\"device_map\") != \"auto\":\n",
        "        model.to(device)\n",
        "    else:\n",
        "        print(\"Model loaded with device_map='auto' (quantization)\")\n",
        "\n",
        "    # Clear cache before training\n",
        "    if device_type == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "        import gc\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB total\")\n",
        "            print(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "            print(f\"GPU memory reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
        "\n",
        "    train_dataset = ConversationDataset(train_samples, tokenizer, cfg.max_seq_length)\n",
        "    valid_dataset = ConversationDataset(valid_samples, tokenizer, cfg.max_seq_length) if valid_samples else None\n",
        "\n",
        "    collate_fn = lambda batch: collate_conversations(batch, tokenizer.pad_token_id)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=cfg.per_device_batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=False,\n",
        "    )\n",
        "    valid_loader = (\n",
        "        DataLoader(\n",
        "            valid_dataset,\n",
        "            batch_size=max(1, cfg.per_device_batch_size),\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "        )\n",
        "        if valid_dataset\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    steps_per_epoch = max(1, math.ceil(len(train_loader) / cfg.grad_accum_steps))\n",
        "    planned_steps = cfg.target_steps\n",
        "    epochs = min(cfg.max_epochs, max(1, math.ceil(planned_steps / steps_per_epoch)))\n",
        "    total_steps = epochs * steps_per_epoch\n",
        "\n",
        "    print(f\"Training plan: epochs={epochs}, steps/epoch≈{steps_per_epoch}, total_steps={total_steps}\")\n",
        "\n",
        "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    optimizer = None\n",
        "    galore_handler = None\n",
        "\n",
        "    if GaLoreAdamW is not None:\n",
        "        try:\n",
        "            # Try different GaLoreAdamW API signatures\n",
        "            galore_rank = min(256, max(4, int(cfg.max_seq_length * cfg.galore_rank_ratio)))\n",
        "            try:\n",
        "                # Try with rank as positional or keyword arg\n",
        "                optimizer = GaLoreAdamW(\n",
        "                    trainable_params,\n",
        "                    lr=cfg.learning_rate,\n",
        "                    betas=(0.9, 0.999),\n",
        "                    eps=1e-8,\n",
        "                    weight_decay=cfg.weight_decay,\n",
        "                    rank=galore_rank,\n",
        "                )\n",
        "                print(f\"GaLoreAdamW initialized with rank={galore_rank}\")\n",
        "            except TypeError:\n",
        "                # Try without rank parameter (some versions don't need it)\n",
        "                try:\n",
        "                    optimizer = GaLoreAdamW(\n",
        "                        trainable_params,\n",
        "                        lr=cfg.learning_rate,\n",
        "                        betas=(0.9, 0.999),\n",
        "                        eps=1e-8,\n",
        "                        weight_decay=cfg.weight_decay,\n",
        "                    )\n",
        "                    print(f\"GaLoreAdamW initialized (without rank parameter)\")\n",
        "                except Exception as e2:\n",
        "                    raise e2\n",
        "            galore_handler = None\n",
        "        except Exception as init_err:\n",
        "            print(f\"WARNING: GaLoreAdamW initialization failed: {init_err}. Using fallback projection.\")\n",
        "\n",
        "    if optimizer is None:\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            trainable_params,\n",
        "            lr=cfg.learning_rate,\n",
        "            weight_decay=cfg.weight_decay,\n",
        "            betas=(0.9, 0.999),\n",
        "        )\n",
        "        galore_handler = project_gradients_low_rank\n",
        "\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=min(cfg.warmup_steps, total_steps // 2),\n",
        "        num_training_steps=total_steps,\n",
        "    )\n",
        "\n",
        "    global_step = 0\n",
        "    running_loss = 0.0\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    nan_count = 0\n",
        "    max_nan_steps = 5  # Stop if NaN persists for 5 steps\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "        for step, batch in enumerate(train_loader):\n",
        "            model.train()\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            with torch.autocast(device_type=device_type, dtype=autocast_dtype, enabled=device_type != \"cpu\"):\n",
        "                outputs = model(**batch)\n",
        "                loss = outputs.loss / cfg.grad_accum_steps\n",
        "\n",
        "            # Check for NaN loss\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                nan_count += 1\n",
        "                print(f\"WARNING: NaN/Inf loss detected at step {global_step} (count: {nan_count}/{max_nan_steps})\")\n",
        "                if nan_count >= max_nan_steps:\n",
        "                    print(\"ERROR: Too many NaN losses, stopping training. Try:\")\n",
        "                    print(\"   1. Reduce learning rate (set LEARNING_RATE=1e-5)\")\n",
        "                    print(\"   2. Reduce batch size (set BATCH_SIZE=1)\")\n",
        "                    print(\"   3. Reduce sequence length (set MAX_SEQ_LENGTH=512)\")\n",
        "                    return {\"error\": \"NaN loss\", \"steps\": global_step}\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                continue\n",
        "            else:\n",
        "                nan_count = 0  # Reset counter on valid loss\n",
        "\n",
        "            loss.backward()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (step + 1) % cfg.grad_accum_steps == 0:\n",
        "                # Gradient clipping to prevent explosion\n",
        "                torch.nn.utils.clip_grad_norm_(trainable_params, cfg.max_grad_norm)\n",
        "\n",
        "                if cfg.galore_project_on and galore_handler is not None:\n",
        "                    galore_handler(model, rank_ratio=cfg.galore_rank_ratio)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "                global_step += 1\n",
        "                avg_loss = running_loss\n",
        "                running_loss = 0.0\n",
        "\n",
        "                if global_step % 5 == 0:\n",
        "                    print(f\"   step {global_step}/{total_steps} - loss: {avg_loss:.4f}\")\n",
        "\n",
        "                if global_step >= planned_steps:\n",
        "                    break\n",
        "\n",
        "        if global_step >= planned_steps:\n",
        "            break\n",
        "\n",
        "        val_loss = evaluate_model(model, valid_loader, device, device_type, autocast_dtype)\n",
        "        if val_loss is not None:\n",
        "            print(f\"   Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "    final_val_loss = evaluate_model(model, valid_loader, device, device_type, autocast_dtype)\n",
        "\n",
        "    Path(cfg.adapter_path).mkdir(parents=True, exist_ok=True)\n",
        "    model.to(\"cpu\")\n",
        "    model.save_pretrained(cfg.adapter_path)\n",
        "    tokenizer.save_pretrained(cfg.adapter_path)\n",
        "\n",
        "    metrics = {\n",
        "        \"train_steps\": global_step,\n",
        "        \"validation_loss\": final_val_loss,\n",
        "        \"device\": str(device),\n",
        "        \"dtype\": str(model_dtype),\n",
        "        \"galore_backend\": \"official\" if GaLoreAdamW is not None else \"fallback_svd\",\n",
        "    }\n",
        "\n",
        "    with open(Path(cfg.adapter_path) / \"training_summary.json\", \"w\") as fp:\n",
        "        json.dump(metrics, fp, indent=2)\n",
        "\n",
        "    print(f\"\\nTraining complete! Adapter saved to: {cfg.adapter_path}\")\n",
        "    if final_val_loss is not None:\n",
        "        print(f\"Final validation loss: {final_val_loss:.4f}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def fine_tune_model():\n",
        "    \"\"\"Execute DoRA + GaLore fine-tuning\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Starting DoRA + GaLore Fine-tuning (PyTorch + PEFT)\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    cfg = TrainingConfig()\n",
        "\n",
        "    # Print directory information (shows where files will be created)\n",
        "    print(f\"Working Directory: {cfg._script_dir}\")\n",
        "    print(f\"Training Data Directory: {cfg.data_dir}\")\n",
        "    print(f\"Model Save Directory: {cfg.adapter_path}\")\n",
        "    print()\n",
        "\n",
        "    # Create necessary directories\n",
        "    cfg.data_dir.mkdir(parents=True, exist_ok=True)\n",
        "    Path(cfg.adapter_path).mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Created directory: {cfg.data_dir}\")\n",
        "    print(f\"Created directory: {cfg.adapter_path}\")\n",
        "    print()\n",
        "\n",
        "    resume_text = extract_resume_info()\n",
        "    if not resume_text:\n",
        "        return False\n",
        "\n",
        "    training_samples = generate_training_data(resume_text)\n",
        "\n",
        "    total = len(training_samples)\n",
        "    valid_size = 1 if total < 5 else max(1, int(0.1 * total))\n",
        "    train_samples = training_samples[:-valid_size] if total > valid_size else training_samples\n",
        "    valid_samples = training_samples[-valid_size:] if total > valid_size else []\n",
        "\n",
        "    train_path = str(cfg.data_dir / \"train.jsonl\")\n",
        "    valid_path = str(cfg.data_dir / \"valid.jsonl\")\n",
        "\n",
        "    create_training_jsonl(train_samples, train_path)\n",
        "    create_training_jsonl(valid_samples, valid_path)\n",
        "\n",
        "    # Print complete training data directory information\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Training Data Summary\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Training Data Directory: {cfg.data_dir}\")\n",
        "    print(f\"   ├─ Train file: {train_path}\")\n",
        "    print(f\"   │  └─ Samples: {len(train_samples)}\")\n",
        "    print(f\"   └─ Validation file: {valid_path}\")\n",
        "    print(f\"      └─ Samples: {len(valid_samples)}\")\n",
        "    print(f\"Model Save Directory: {cfg.adapter_path}\")\n",
        "    print(f\"Total Training Samples: {len(train_samples)}\")\n",
        "    print(f\"Total Validation Samples: {len(valid_samples)}\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    if len(train_samples) == 0:\n",
        "        print(\"ERROR: Training data is empty, terminating training\")\n",
        "        return False\n",
        "\n",
        "    metrics = train_with_dora_galore(train_samples, valid_samples, cfg)\n",
        "    return metrics\n",
        "\n",
        "def extract_resume_info():\n",
        "    \"\"\"Extract raw text from PDF resume (supports local and Google Colab)\"\"\"\n",
        "    print(\"Reading resume PDF...\")\n",
        "\n",
        "    # Check if path specified via environment variable\n",
        "    resume_path = os.getenv(\"RESUME_PDF_PATH\", None)\n",
        "\n",
        "    # If not specified, try common locations\n",
        "    if not resume_path or not os.path.exists(resume_path):\n",
        "        work_dir = _get_working_directory()\n",
        "        possible_paths = [\n",
        "            # Check current working directory\n",
        "            work_dir / \"F_CV_Chen_Linlin_formal_cv_itp.pdf\",\n",
        "            work_dir / \"resume.pdf\",\n",
        "            work_dir / \"CV.pdf\",\n",
        "            # Check content subdirectory (common in Colab)\n",
        "            work_dir / \"content\" / \"F_CV_Chen_Linlin_formal_cv_itp.pdf\",\n",
        "            work_dir / \"content\" / \"resume.pdf\",\n",
        "            work_dir / \"content\" / \"CV.pdf\",\n",
        "            # Check Google Drive if mounted\n",
        "            Path(\"/content/drive/MyDrive/F_CV_Chen_Linlin_formal_cv_itp.pdf\"),\n",
        "            Path(\"/content/drive/MyDrive/resume.pdf\"),\n",
        "            Path(\"/content/drive/MyDrive/CV.pdf\"),\n",
        "            # macOS default location\n",
        "            Path(\"/Users/oww/Dropbox/F_CV_Chen_Linlin_formal_cv_itp.pdf\"),\n",
        "            # Colab root directory\n",
        "            Path(\"/content/F_CV_Chen_Linlin_formal_cv_itp.pdf\"),\n",
        "            Path(\"/content/resume.pdf\"),\n",
        "        ]\n",
        "\n",
        "        for path in possible_paths:\n",
        "            if path.exists():\n",
        "                resume_path = str(path)\n",
        "                print(f\"Found resume file: {resume_path}\")\n",
        "                break\n",
        "\n",
        "        if not resume_path or not os.path.exists(resume_path):\n",
        "            print(f\"ERROR: Resume file not found. Please place your resume PDF in one of the following locations:\")\n",
        "            for path in possible_paths:\n",
        "                print(f\"   - {path}\")\n",
        "            print(f\"   Or set environment variable: RESUME_PDF_PATH=/path/to/resume.pdf\")\n",
        "            return None\n",
        "\n",
        "    try:\n",
        "        import PyPDF2\n",
        "        print(f\"Reading: {resume_path}\")\n",
        "        with open(resume_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            text = \"\"\n",
        "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
        "                page_text = page.extract_text()\n",
        "                text += page_text + \"\\n\"\n",
        "                print(f\"   Read page {page_num} ({len(page_text)} characters)\")\n",
        "\n",
        "        total_chars = len(text)\n",
        "        total_words = len(text.split())\n",
        "        print(f\"Resume extraction complete: {total_chars} characters, {total_words} words, {len(text.splitlines())} lines\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to read PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    result = fine_tune_model()\n",
        "    if result:\n",
        "        print(\"\\nFine-tuning successful! You can now use the fine-tuned model\")\n",
        "        print(f\"Training summary: {json.dumps(result, indent=2)}\")\n",
        "    else:\n",
        "        print(\"\\nWARNING: Fine-tuning failed, suggest using default deepseek-coder:latest\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **README**\n",
        "### - Load adaptor to llm model\n",
        "```\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "# 1. Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "\n",
        "# 2. Load adapter\n",
        "model = PeftModel.from_pretrained(base_model, \"./fine-tuned-model\")\n",
        "\n",
        "# 3. Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./fine-tuned-model\")\n",
        "\n",
        "# 4. Finished\n",
        "inputs = tokenizer(\"question\", return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs)\n",
        "print(tokenizer.decode(outputs[0]))\n",
        "```"
      ],
      "metadata": {
        "id": "PmPn7uwvTTsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tD4C4xalTTZU"
      }
    }
  ]
}